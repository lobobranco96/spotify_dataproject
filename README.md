# ğŸµ Pipeline ETL com API do Spotify, Airflow e Spark

Este repositÃ³rio contÃ©m uma pipeline de dados para coletar, transformar e armazenar dados da API do Spotify utilizando **Apache Airflow** como orquestrador, **Apache Spark** para processamento distribuÃ­do e **MinIO** como Data Lake. O ambiente Ã© totalmente **dockerizado**, garantindo facilidade na configuraÃ§Ã£o e escalabilidade.

## ğŸ—ï¸ Arquitetura

1ï¸âƒ£ **Coleta de dados**: Airflow executa uma DAG que faz requisiÃ§Ãµes Ã  API do Spotify.  
2ï¸âƒ£ **Armazenamento inicial (Raw Layer)**: Os dados coletados sÃ£o armazenados em um bucket no MinIO.  
3ï¸âƒ£ **Processamento com Spark**: Airflow dispara um job Spark via `SparkSubmitOperator` para transformar os dados.  
4ï¸âƒ£ **Armazenamento Processado (Processed Layer)**: Os dados transformados sÃ£o carregados em outro bucket no MinIO.

## ğŸ› ï¸ Tecnologias Utilizadas

- **Apache Airflow** â†’ OrquestraÃ§Ã£o das tarefas  
- **Apache Spark** â†’ Processamento distribuÃ­do  
- **MinIO** â†’ Armazenamento de dados (Data Lake)  
- **Docker** â†’ Gerenciamento do ambiente  
- **Spotify API** â†’ Fonte dos dados

## ğŸ“‚ Estrutura do Projeto

```bash
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Spark/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ Airflow/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ include/
â”‚   â””â”€â”€ imgs/                 # Imagens e diagramas do projeto
â”œâ”€â”€ mnt/
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”‚   â”œâ”€â”€ etl_pipeline.py
â”‚   â”‚   â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ fetch_spotify_data.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ plugins/
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ minio/
â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ spark_job/
â”‚   â”‚   â”œâ”€â”€ code/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ parquet_writer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ spark_session.py
â”‚   â”‚   â”‚   â”œâ”€â”€ spotify_transformation.py
â”‚   â”‚   â”œâ”€â”€ jars/
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ orchestration.yml
â”‚   â”‚   â”œâ”€â”€ processing.yml
â”‚   â”‚   â”œâ”€â”€ storage.yml
â”‚   â”‚   â”œâ”€â”€ conf/
â”‚   â”‚   â”‚   â””â”€â”€ credentials.conf
â”œâ”€â”€ README.md                  # DocumentaÃ§Ã£o do projeto
â””â”€â”€ .env                       # ConfiguraÃ§Ãµes sensÃ­veis (API Keys, URLs, etc.)

## ğŸš€ Como Executar

1ï¸âƒ£ **Clone o repositÃ³rio**:

```bash
git clone https://github.com/seu-usuario/etl-spotify-airflow.git
cd etl-spotify-airflow

2ï¸âƒ£ Configure as variÃ¡veis de ambiente

Crie um arquivo .env na raiz do projeto e adicione:

SPOTIFY_CLIENT_ID=your_client_id
SPOTIFY_CLIENT_SECRET=your_client_secret
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=password
MINIO_DOMAIN=storage
MINIO_REGION_NAME=us-east-1
AWS_REGION=us-east-1
AWS_DEFAULT_REGION=us-east-1
AWS_ACCESS_KEY_ID=admin
AWS_SECRET_ACCESS_KEY=password
S3_ENDPOINT=http://minio:9000

3ï¸âƒ£ Suba o ambiente Docker

docker compose -f services/orchestration.yml up -d --build
docker compose -f services/processing.yml up -d --build
docker compose -f services/storage.yml up -d 

Isso iniciarÃ¡ os serviÃ§os do Airflow, Spark e MinIO.

4ï¸âƒ£ Acesse a interface do Airflow

Abra http://localhost:8080 e ative a DAG etl_pipeline.

5ï¸âƒ£ Verifique os dados no MinIO

Acesse http://localhost:9000 com as credenciais padrÃ£o (minioadmin/minioadmin) para verificar os buckets raw e processed.

ğŸ“Š Fluxo de ExecuÃ§Ã£o

1ï¸âƒ£ IngestÃ£o: A DAG do Airflow coleta dados da API do Spotify e salva no MinIO (raw/)
2ï¸âƒ£ TransformaÃ§Ã£o: Spark processa os dados do raw/, aplicando limpeza e estruturaÃ§Ã£o.
3ï¸âƒ£ Carga: Dados processados sÃ£o armazenados no bucket processed/.

## ğŸ“Œ PrÃ³ximos Passos

- **Carregar os dados em um Banco de dados**: ApÃ³s o processamento, os dados podem ser carregados em um banco de dados relacional ou NoSQL para consultas e visualizaÃ§Ã£o.
- **Criar um dashboard para visualizaÃ§Ã£o dos dados**: Um dashboard pode ser construÃ­do com ferramentas como Power BI, Tableau ou atÃ© mesmo uma aplicaÃ§Ã£o customizada para apresentar os dados de forma visual.

## ğŸ’¡ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Se vocÃª tiver alguma sugestÃ£o, melhorias ou correÃ§Ãµes, sinta-se Ã  vontade para abrir uma **issue** ou enviar um **Pull Request (PR)**. Seu feedback Ã© importante para melhorar o projeto!

## ğŸ“© Contato

## ğŸ“š LicenÃ§a

Este projeto estÃ¡ licenciado sob a [MIT License](LICENSE).

---

Obrigado por conferir o projeto! ğŸš€
